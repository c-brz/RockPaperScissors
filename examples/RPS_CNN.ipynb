{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc4d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.12).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/drgfreeman/rockpaperscissors?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306M/306M [00:28<00:00, 11.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/christina/.cache/kagglehub/datasets/drgfreeman/rockpaperscissors/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"drgfreeman/rockpaperscissors\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb379511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class HandVisualRepresentations:\n",
    "    def __init__(self):\n",
    "        # Initialize MediaPipe hands\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load and preprocess image\"\"\"\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb\n",
    "    \n",
    "    def raw_rgb_representation(self, image):\n",
    "        \"\"\"Raw RGB pixels - baseline representation\"\"\"\n",
    "        # Resize to standard size\n",
    "        resized = cv2.resize(image, (224, 224))\n",
    "        # Normalize to [0, 1]\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized\n",
    "    \n",
    "    def hand_segmentation_mask(self, image):\n",
    "        \"\"\"Create hand segmentation mask using color-based thresholding\"\"\"\n",
    "        # Convert to HSV for better skin color detection\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Define skin color range (adjust based on your dataset)\n",
    "        lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "        upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "        \n",
    "        # Morphological operations to clean up mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Find largest contour (assumed to be hand)\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            mask = np.zeros_like(mask)\n",
    "            cv2.fillPoly(mask, [largest_contour], 255)\n",
    "        \n",
    "        return mask.astype(np.float32) / 255.0\n",
    "    \n",
    "    def improved_hand_segmentation(self, image):\n",
    "        \"\"\"Improved segmentation using K-means clustering\"\"\"\n",
    "        # Reshape image for K-means\n",
    "        pixel_values = image.reshape((-1, 3))\n",
    "        pixel_values = np.float32(pixel_values)\n",
    "        \n",
    "        # Apply K-means clustering\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "        k = 3  # Assuming background, hand, and shadows/variations\n",
    "        _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "        \n",
    "        # Convert back to uint8 and reshape\n",
    "        centers = np.uint8(centers)\n",
    "        segmented_image = centers[labels.flatten()]\n",
    "        segmented_image = segmented_image.reshape(image.shape)\n",
    "        \n",
    "        # Find the cluster that represents the hand (usually the one with mid-range intensity)\n",
    "        cluster_means = [np.mean(centers[i]) for i in range(k)]\n",
    "        hand_cluster = np.argsort(cluster_means)[1]  # Middle intensity cluster\n",
    "        \n",
    "        # Create binary mask for hand cluster\n",
    "        mask = (labels.flatten() == hand_cluster).astype(np.uint8) * 255\n",
    "        mask = mask.reshape(image.shape[:2])\n",
    "        \n",
    "        # Clean up mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        return mask.astype(np.float32) / 255.0\n",
    "    \n",
    "    def edge_representation(self, image):\n",
    "        \"\"\"Extract edge maps of the hand\"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "        \n",
    "        # Canny edge detection\n",
    "        edges = cv2.Canny(blurred, 50, 150)\n",
    "        \n",
    "        # Optional: Dilate edges to make them more prominent\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "        \n",
    "        return edges.astype(np.float32) / 255.0\n",
    "    \n",
    "    def hand_landmarks_representation(self, image):\n",
    "        \"\"\"Extract MediaPipe hand landmarks\"\"\"\n",
    "        results = self.hands.process(image)\n",
    "        \n",
    "        landmarks_array = np.zeros((21, 2))  # 21 landmarks, x,y coordinates\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]  # First hand\n",
    "            \n",
    "            for i, landmark in enumerate(hand_landmarks.landmark):\n",
    "                landmarks_array[i, 0] = landmark.x * image.shape[1]  # x coordinate\n",
    "                landmarks_array[i, 1] = landmark.y * image.shape[0]  # y coordinate\n",
    "        \n",
    "        return landmarks_array\n",
    "    \n",
    "    def create_landmark_image(self, image, landmarks):\n",
    "        \"\"\"Create visual representation of landmarks\"\"\"\n",
    "        landmark_image = np.zeros_like(image)\n",
    "        \n",
    "        if np.any(landmarks):  # If landmarks were detected\n",
    "            # Draw landmarks as circles\n",
    "            for landmark in landmarks:\n",
    "                if landmark[0] > 0 and landmark[1] > 0:  # Valid landmark\n",
    "                    cv2.circle(landmark_image, \n",
    "                             (int(landmark[0]), int(landmark[1])), \n",
    "                             5, (255, 255, 255), -1)\n",
    "            \n",
    "            # Draw connections between landmarks\n",
    "            connections = [\n",
    "                # Thumb\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                # Index finger\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                # Middle finger\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                # Ring finger\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                # Pinky\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20)\n",
    "            ]\n",
    "            \n",
    "            for connection in connections:\n",
    "                start_point = landmarks[connection[0]]\n",
    "                end_point = landmarks[connection[1]]\n",
    "                if (start_point[0] > 0 and start_point[1] > 0 and \n",
    "                    end_point[0] > 0 and end_point[1] > 0):\n",
    "                    cv2.line(landmark_image,\n",
    "                           (int(start_point[0]), int(start_point[1])),\n",
    "                           (int(end_point[0]), int(end_point[1])),\n",
    "                           (255, 255, 255), 2)\n",
    "        \n",
    "        return landmark_image\n",
    "    \n",
    "    def optical_flow_representation(self, prev_frame, curr_frame):\n",
    "        \"\"\"Compute dense optical flow between two frames\"\"\"\n",
    "        # Convert to grayscale\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, None, None)\n",
    "        \n",
    "        # Alternative: Dense optical flow\n",
    "        flow_dense = cv2.calcOpticalFlowFarneback(\n",
    "            prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "        )\n",
    "        \n",
    "        # Convert flow to HSV for visualization\n",
    "        h, w = flow_dense.shape[:2]\n",
    "        fx, fy = flow_dense[:,:,0], flow_dense[:,:,1]\n",
    "        \n",
    "        # Convert to polar coordinates\n",
    "        mag, ang = cv2.cartToPolar(fx, fy)\n",
    "        \n",
    "        # Create HSV image\n",
    "        hsv_flow = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        hsv_flow[:,:,0] = ang * 180 / np.pi / 2  # Hue represents direction\n",
    "        hsv_flow[:,:,1] = 255  # Full saturation\n",
    "        hsv_flow[:,:,2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)  # Value represents magnitude\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        flow_rgb = cv2.cvtColor(hsv_flow, cv2.COLOR_HSV2RGB)\n",
    "        \n",
    "        return flow_rgb.astype(np.float32) / 255.0, flow_dense\n",
    "    \n",
    "    def temporal_difference(self, prev_frame, curr_frame):\n",
    "        \"\"\"Compute temporal difference between frames\"\"\"\n",
    "        # Convert to grayscale\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n",
    "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Compute absolute difference\n",
    "        diff = cv2.absdiff(prev_gray, curr_gray)\n",
    "        \n",
    "        # Threshold to remove noise\n",
    "        _, diff_thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        return diff_thresh.astype(np.float32) / 255.0\n",
    "    \n",
    "    def process_rps_image(self, image_path):\n",
    "        \"\"\"Process a single RPS image and extract all representations\"\"\"\n",
    "        image = self.load_image(image_path)\n",
    "        \n",
    "        representations = {}\n",
    "        \n",
    "        # Raw RGB\n",
    "        representations['raw_rgb'] = self.raw_rgb_representation(image)\n",
    "        \n",
    "        # Segmentation masks\n",
    "        representations['seg_mask'] = self.hand_segmentation_mask(image)\n",
    "        representations['seg_mask_improved'] = self.improved_hand_segmentation(image)\n",
    "        \n",
    "        # Edge representation\n",
    "        representations['edges'] = self.edge_representation(image)\n",
    "        \n",
    "        # Hand landmarks\n",
    "        landmarks = self.hand_landmarks_representation(image)\n",
    "        representations['landmarks'] = landmarks\n",
    "        representations['landmark_image'] = self.create_landmark_image(image, landmarks)\n",
    "        \n",
    "        return representations, image\n",
    "\n",
    "# Example usage and visualization\n",
    "def visualize_representations(image_path):\n",
    "    \"\"\"Visualize all representations for a single image\"\"\"\n",
    "    processor = HandVisualRepresentations()\n",
    "    representations, original_image = processor.process_rps_image(image_path)\n",
    "    \n",
    "    # Create subplot for visualization\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.suptitle(f'Hand Representations for {Path(image_path).name}', fontsize=16)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original_image)\n",
    "    axes[0, 0].set_title('Original RGB')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Raw RGB (resized)\n",
    "    axes[0, 1].imshow(representations['raw_rgb'])\n",
    "    axes[0, 1].set_title('Processed RGB')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Basic segmentation\n",
    "    axes[0, 2].imshow(representations['seg_mask'], cmap='gray')\n",
    "    axes[0, 2].set_title('Basic Segmentation')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Improved segmentation\n",
    "    axes[0, 3].imshow(representations['seg_mask_improved'], cmap='gray')\n",
    "    axes[0, 3].set_title('K-means Segmentation')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Edge representation\n",
    "    axes[1, 0].imshow(representations['edges'], cmap='gray')\n",
    "    axes[1, 0].set_title('Edge Map')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Landmarks visualization\n",
    "    axes[1, 1].imshow(representations['landmark_image'])\n",
    "    axes[1, 1].set_title('Hand Landmarks')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Show landmarks on original\n",
    "    landmark_overlay = original_image.copy()\n",
    "    landmarks = representations['landmarks']\n",
    "    if np.any(landmarks):\n",
    "        for landmark in landmarks:\n",
    "            if landmark[0] > 0 and landmark[1] > 0:\n",
    "                cv2.circle(landmark_overlay, (int(landmark[0]), int(landmark[1])), 3, (255, 0, 0), -1)\n",
    "    \n",
    "    axes[1, 2].imshow(landmark_overlay)\n",
    "    axes[1, 2].set_title('Landmarks on Original')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Print landmark statistics\n",
    "    if np.any(landmarks):\n",
    "        axes[1, 3].text(0.1, 0.5, f'Landmarks detected: {np.sum(np.any(landmarks, axis=1))}/21\\n\\n'\n",
    "                                   f'Hand span (pixels):\\n'\n",
    "                                   f'Width: {np.max(landmarks[:, 0]) - np.min(landmarks[:, 0]):.1f}\\n'\n",
    "                                   f'Height: {np.max(landmarks[:, 1]) - np.min(landmarks[:, 1]):.1f}',\n",
    "                        transform=axes[1, 3].transAxes, fontsize=10, verticalalignment='center')\n",
    "    else:\n",
    "        axes[1, 3].text(0.1, 0.5, 'No landmarks detected', \n",
    "                        transform=axes[1, 3].transAxes, fontsize=10, verticalalignment='center')\n",
    "    \n",
    "    axes[1, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return representations\n",
    "\n",
    "# PyTorch dataset class for different representations\n",
    "class RPSRepresentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, representation_type='raw_rgb', transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.representation_type = representation_type\n",
    "        self.transform = transform\n",
    "        self.processor = HandVisualRepresentations()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        representations, _ = self.processor.process_rps_image(image_path)\n",
    "        \n",
    "        if self.representation_type == 'raw_rgb':\n",
    "            data = representations['raw_rgb']\n",
    "        elif self.representation_type == 'seg_mask':\n",
    "            data = representations['seg_mask']\n",
    "            data = np.stack([data, data, data], axis=-1)  # Convert to 3-channel\n",
    "        elif self.representation_type == 'edges':\n",
    "            data = representations['edges']\n",
    "            data = np.stack([data, data, data], axis=-1)  # Convert to 3-channel\n",
    "        elif self.representation_type == 'landmarks':\n",
    "            data = representations['landmarks'].flatten()  # Flatten to 1D array\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown representation type: {self.representation_type}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            if self.representation_type != 'landmarks':\n",
    "                data = self.transform(data)\n",
    "            \n",
    "        return data, label\n",
    "\n",
    "# Example: How to use different representations in training\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage (you'll need to adapt paths to your dataset)\n",
    "    sample_image_path = \"path/to/your/rps/image.jpg\"  # Replace with actual path\n",
    "    \n",
    "    print(\"Processing hand representations...\")\n",
    "    representations = visualize_representations(sample_image_path)\n",
    "    \n",
    "    print(f\"Representation shapes:\")\n",
    "    for key, value in representations.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"{key}: {value.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
