{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.hand_visualizations import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATASET_PATH = \"../2/rps-cv-images\"\n",
    "\n",
    "# --- REPRESENTATIONS TO TEST ---\n",
    "REPRESENTATIONS = ['raw_rgb', 'landmarks', 'edges', 'seg_mask']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07329a2a",
   "metadata": {},
   "source": [
    "### ML Classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d73d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    for rep in REPRESENTATIONS:\n",
    "        print(f\"\\n=== Representation: {rep} ===\")\n",
    "        \n",
    "        processor = HandVisualRepresentations()\n",
    "        X, y = [], []\n",
    "        for img, lab in zip(image_paths, labels_encoded):\n",
    "            representations, _ = processor.process_rps_image(img)\n",
    "            feat = representations.get(rep)\n",
    "            if feat is not None:\n",
    "                if rep == 'landmarks':\n",
    "                    X.append(feat.flatten())\n",
    "                elif rep == 'raw_rgb':\n",
    "                    X.append(feat.flatten())\n",
    "                else:\n",
    "                    X.append(feat.flatten())\n",
    "                y.append(lab)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        \n",
    "        if len(X) > 10:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "            scaler = StandardScaler()\n",
    "            if rep != 'raw_rgb':  # Don't scale images\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "            accs = {}\n",
    "            for name, model in ML_MODELS.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                accs[name] = acc\n",
    "                print(f\"ML {name} accuracy: {acc:.3f}\")\n",
    "            results_ml[rep] = accs\n",
    "        results_dl[rep] = {'train_loss': train_losses, 'test_acc': test_accs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(10,6))\n",
    "model_names = list(ML_MODELS.keys())\n",
    "for rep in REPRESENTATIONS:\n",
    "    if rep in results_ml:\n",
    "        accs = [results_ml[rep][name] for name in model_names]\n",
    "        plt.plot(model_names, accs, label=f\"{rep}\", marker='o')\n",
    "plt.title(\"ML Model Accuracies per Representation\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies as grouped barplot\n",
    "plt.figure(figsize=(10,6))\n",
    "model_names = list(ML_MODELS.keys())\n",
    "bar_width = 0.18\n",
    "x = np.arange(len(model_names))\n",
    "for i, rep in enumerate(REPRESENTATIONS):\n",
    "    if rep in results_ml:\n",
    "        accs = [results_ml[rep][name] for name in model_names]\n",
    "        colors = sns.color_palette(\"Set2\", len(REPRESENTATIONS))\n",
    "        plt.bar(x + i*bar_width, accs, width=bar_width, label=f\"{rep}\", color=colors[i])\n",
    "        for j, v in enumerate(accs):\n",
    "            plt.text(x[j] + i*bar_width, v + 0.01, f\"{v:.2f}\", ha='center', va='bottom', fontsize=9)\n",
    "plt.xticks(x + bar_width*1.5, model_names)\n",
    "plt.title(\"ML Model Accuracies per Representation\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c23be",
   "metadata": {},
   "source": [
    "### DL Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af043e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- DATASET CLASS ---\n",
    "class HandDataset2(Dataset):\n",
    "    def __init__(self, image_paths, labels, representation, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.representation = representation\n",
    "        self.transform = transform\n",
    "        self.processor = HandVisualRepresentations()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            representations, _ = self.processor.process_rps_image(img_path)\n",
    "            rep = representations.get(self.representation)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "            rep = None\n",
    "            \n",
    "        # Handle missing representations\n",
    "        if rep is None:\n",
    "            if self.representation == 'landmarks':\n",
    "                rep = np.zeros(42)\n",
    "            elif self.representation == 'raw_rgb':\n",
    "                rep = np.zeros((224, 224, 3))\n",
    "            else:\n",
    "                rep = np.zeros((224, 224))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        if self.representation == 'landmarks':\n",
    "            return torch.tensor(rep, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        elif self.representation == 'raw_rgb':\n",
    "            if self.transform:\n",
    "                rep = self.transform(rep)\n",
    "            else:\n",
    "                rep = torch.tensor(rep.transpose(2, 0, 1), dtype=torch.float32) / 255.\n",
    "            return rep, torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            rep = torch.tensor(rep, dtype=torch.float32)\n",
    "            if rep.ndim == 2:\n",
    "                rep = rep.unsqueeze(0)\n",
    "            return rep, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "class HandDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, representation, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.representation = representation\n",
    "        self.transform = transform\n",
    "        self.processor = HandVisualRepresentations()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            representations, _ = self.processor.process_rps_image(img_path)\n",
    "            rep = representations.get(self.representation)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: {e}\")\n",
    "            rep = None\n",
    "            \n",
    "        # Handle missing representations\n",
    "        if rep is None:\n",
    "            if self.representation == 'landmarks':\n",
    "                rep = np.zeros(42)  # 21 landmarks Ã— 2 coordinates\n",
    "            elif self.representation == 'raw_rgb':\n",
    "                rep = np.zeros((224, 224, 3))\n",
    "            else:\n",
    "                rep = np.zeros((224, 224))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        if self.representation == 'landmarks':\n",
    "            # Ensure landmarks are properly shaped (21, 2) and flatten to (42,)\n",
    "            if rep is not None:\n",
    "                rep = np.array(rep)\n",
    "                if rep.shape == (21, 3):  # If it has x, y, z coordinates\n",
    "                    rep = rep[:, :2]  # Take only x, y coordinates\n",
    "                elif rep.shape == (21, 2):  # Already correct shape\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected landmark shape {rep.shape}, using zeros\")\n",
    "                    rep = np.zeros((21, 2))\n",
    "                \n",
    "                # Flatten to 42 values\n",
    "                rep = rep.flatten()\n",
    "                \n",
    "                # Ensure exactly 42 values\n",
    "                if len(rep) != 42:\n",
    "                    print(f\"Warning: Landmark length {len(rep)}, padding/truncating to 42\")\n",
    "                    if len(rep) < 42:\n",
    "                        rep = np.pad(rep, (0, 42 - len(rep)), 'constant', constant_values=0)\n",
    "                    else:\n",
    "                        rep = rep[:42]\n",
    "            \n",
    "            return torch.tensor(rep, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "            \n",
    "        elif self.representation == 'raw_rgb':\n",
    "            if self.transform:\n",
    "                rep = self.transform(rep)\n",
    "            else:\n",
    "                rep = torch.tensor(rep.transpose(2, 0, 1), dtype=torch.float32) / 255.\n",
    "            return rep, torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            rep = torch.tensor(rep, dtype=torch.float32)\n",
    "            if rep.ndim == 2:\n",
    "                rep = rep.unsqueeze(0)\n",
    "            return rep, torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "# --- DL ARCHITECTURES ---\n",
    "\n",
    "# 1. Simple MLP for landmarks\n",
    "class LandmarkMLP(nn.Module):\n",
    "    def __init__(self, input_size=42, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 2. Deep MLP for landmarks\n",
    "class DeepLandmarkMLP(nn.Module):\n",
    "    def __init__(self, input_size=42, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))  # Fixed output size\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Now always 64*7*7 = 3136\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.adaptive_pool(x)  # Ensures consistent size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))  # Fixed output size\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)  # Now always 256*4*4 = 4096\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.adaptive_pool(x)  # Ensures consistent size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = ResNetBlock(64, 64)\n",
    "        self.layer2 = ResNetBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResNetBlock(128, 256, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=5, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        test_accs.append(accuracy)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Loss={train_losses[-1]:.4f}, Acc={accuracy:.3f}\")\n",
    "    \n",
    "    return train_losses, test_accs\n",
    "\n",
    "\n",
    "def run_dl_experiments():\n",
    "    # Load dataset\n",
    "    image_paths, labels, _ = explore_rps_dataset(DATASET_PATH)\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "        image_paths, labels_encoded, test_size=0.2, stratify=labels_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for rep in REPRESENTATIONS:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"REPRESENTATION: {rep}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Define architectures for this representation\n",
    "        if rep == 'landmarks':\n",
    "            architectures = {\n",
    "                'Simple MLP': LandmarkMLP(input_size=42, num_classes=3),\n",
    "                'Deep MLP': DeepLandmarkMLP(input_size=42, num_classes=3)\n",
    "            }\n",
    "            transform = None\n",
    "            batch_size = 32\n",
    "        else:\n",
    "            in_channels = 3 if rep == 'raw_rgb' else 1\n",
    "            architectures = {\n",
    "                'Simple CNN': SimpleCNN(in_channels=in_channels, num_classes=3),\n",
    "                'Deep CNN': DeepCNN(in_channels=in_channels, num_classes=3),\n",
    "                'ResNet CNN': ResNetCNN(in_channels=in_channels, num_classes=3)\n",
    "            }\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor() if rep == 'raw_rgb' else transforms.Lambda(lambda x: x),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3) if rep == 'raw_rgb' else transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            batch_size = 16\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = HandDataset(X_train_paths, y_train, rep, transform)\n",
    "        test_dataset = HandDataset(X_test_paths, y_test, rep, transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        results[rep] = {}\n",
    "        \n",
    "        for arch_name, model in architectures.items():\n",
    "            print(f\"\\nTraining {arch_name}...\")\n",
    "            train_losses, test_accs = train_model(model, train_loader, test_loader, epochs=5, device=device)\n",
    "            results[rep][arch_name] = {\n",
    "                'train_losses': train_losses,\n",
    "                'test_accs': test_accs,\n",
    "                'final_acc': test_accs[-1]\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    # Plot 1: Final accuracies comparison\n",
    "    num_reps = len(REPRESENTATIONS)\n",
    "    fig_rows = 2\n",
    "    fig_cols = max(2, (num_reps + 1) // 2 + 1)  # Adjust columns based on representations\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    all_results = []\n",
    "    for rep in results:\n",
    "        for arch in results[rep]:\n",
    "            all_results.append({\n",
    "                'Representation': rep,\n",
    "                'Architecture': arch,\n",
    "                'Accuracy': results[rep][arch]['final_acc']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    plt.subplot(fig_rows, fig_cols, 1)\n",
    "    sns.barplot(data=df, x='Architecture', y='Accuracy', hue='Representation')\n",
    "    plt.title('Final Test Accuracy by Architecture and Representation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Plot 2: Training curves for each representation\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for i, rep in enumerate(REPRESENTATIONS):\n",
    "        if rep in results:  # Only plot if we have results for this representation\n",
    "            plt.subplot(fig_rows, fig_cols, i+2)\n",
    "            color_idx = 0\n",
    "            for arch in results[rep]:\n",
    "                epochs = range(1, len(results[rep][arch]['test_accs']) + 1)\n",
    "                plt.plot(epochs, results[rep][arch]['test_accs'], \n",
    "                        label=arch, color=colors[color_idx], marker='o')\n",
    "                color_idx += 1\n",
    "            plt.title(f'{rep} - Test Accuracy over Epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Representation':<15} {'Architecture':<15} {'Final Accuracy':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for rep in results:\n",
    "        for arch in results[rep]:\n",
    "            acc = results[rep][arch]['final_acc']\n",
    "            print(f\"{rep:<15} {arch:<15} {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ce88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/christina/code/RockPaperScissors/2/rps-cv-images\n",
      "Dataset exists: True\n",
      "Total images found: 2188\n",
      "\n",
      "Class distribution:\n",
      "paper: 712 images\n",
      "rock: 726 images\n",
      "scissors: 750 images\n",
      "\n",
      "Sample image paths:\n",
      "paper: /Users/christina/code/RockPaperScissors/2/rps-cv-images/paper/W79peyAyfQqNP1vF.png\n",
      "rock: /Users/christina/code/RockPaperScissors/2/rps-cv-images/rock/foxUXc8WPRDAd6LM.png\n",
      "scissors: /Users/christina/code/RockPaperScissors/2/rps-cv-images/scissors/6TMYdUMhaEWHQOcc.png\n",
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "REPRESENTATION: raw_rgb\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752743246.608841   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1752743246.612763   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "W0000 00:00:1752743246.616749   39758 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752743246.618922   39768 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752743246.621555   39752 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752743246.623469   39768 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752743246.689451   39761 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Simple CNN...\n",
      "Epoch 1/5: Loss=1.0002, Acc=0.852\n",
      "Epoch 2/5: Loss=0.3875, Acc=0.884\n",
      "Epoch 3/5: Loss=0.2619, Acc=0.929\n",
      "Epoch 4/5: Loss=0.1850, Acc=0.941\n",
      "Epoch 5/5: Loss=0.1633, Acc=0.938\n",
      "\n",
      "Training Deep CNN...\n",
      "Epoch 1/5: Loss=1.2677, Acc=0.918\n",
      "Epoch 2/5: Loss=0.2089, Acc=0.943\n",
      "Epoch 3/5: Loss=0.1560, Acc=0.957\n",
      "Epoch 4/5: Loss=0.1466, Acc=0.945\n",
      "Epoch 5/5: Loss=0.0938, Acc=0.984\n",
      "\n",
      "Training ResNet CNN...\n",
      "Epoch 1/5: Loss=0.3528, Acc=0.893\n",
      "Epoch 2/5: Loss=0.1384, Acc=0.904\n",
      "Epoch 3/5: Loss=0.0820, Acc=0.986\n",
      "Epoch 4/5: Loss=0.0524, Acc=0.977\n",
      "Epoch 5/5: Loss=0.0669, Acc=0.986\n",
      "\n",
      "==================================================\n",
      "REPRESENTATION: landmarks\n",
      "==================================================\n",
      "\n",
      "Training Simple MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752746885.658174   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "W0000 00:00:1752746885.665390   68106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1752746885.665653   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "W0000 00:00:1752746885.669623   68106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752746885.670574   68120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752746885.674822   68125 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (672x2 and 42x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- RUN EXPERIMENTS ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_dl_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     plot_results(results)\n",
      "Cell \u001b[0;32mIn[2], line 285\u001b[0m, in \u001b[0;36mrun_dl_experiments\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arch_name, model \u001b[38;5;129;01min\u001b[39;00m architectures\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00march_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m         train_losses, test_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         results[rep][arch_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses,\n\u001b[1;32m    288\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accs\u001b[39m\u001b[38;5;124m'\u001b[39m: test_accs,\n\u001b[1;32m    289\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: test_accs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    290\u001b[0m         }\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[2], line 202\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, epochs, device)\u001b[0m\n\u001b[1;32m    200\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    201\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 202\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m    204\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs587_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs587_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m, in \u001b[0;36mLandmarkMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs587_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs587_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs587_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (672x2 and 42x128)"
     ]
    }
   ],
   "source": [
    "# --- RUN EXPERIMENTS ---\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_dl_experiments()\n",
    "    plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a56f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/christina/code/RockPaperScissors/2/rps-cv-images\n",
      "Dataset exists: True\n",
      "Total images found: 2188\n",
      "\n",
      "Class distribution:\n",
      "paper: 712 images\n",
      "rock: 726 images\n",
      "scissors: 750 images\n",
      "\n",
      "Sample image paths:\n",
      "paper: /Users/christina/code/RockPaperScissors/2/rps-cv-images/paper/W79peyAyfQqNP1vF.png\n",
      "rock: /Users/christina/code/RockPaperScissors/2/rps-cv-images/rock/foxUXc8WPRDAd6LM.png\n",
      "scissors: /Users/christina/code/RockPaperScissors/2/rps-cv-images/scissors/6TMYdUMhaEWHQOcc.png\n",
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "REPRESENTATION: edges\n",
      "==================================================\n",
      "\n",
      "Training Simple CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752750115.336934   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "I0000 00:00:1752750115.342863   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "W0000 00:00:1752750115.347086   97162 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752750115.354933   97171 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752750115.355558   97169 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752750115.362508   97176 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Loss=0.6884, Acc=0.833\n",
      "Epoch 2/5: Loss=0.3965, Acc=0.863\n",
      "Epoch 3/5: Loss=0.3021, Acc=0.870\n",
      "Epoch 4/5: Loss=0.2517, Acc=0.895\n",
      "Epoch 5/5: Loss=0.1986, Acc=0.904\n",
      "\n",
      "Training ResNet CNN...\n",
      "Epoch 1/5: Loss=0.3815, Acc=0.594\n",
      "Epoch 2/5: Loss=0.2463, Acc=0.523\n",
      "Epoch 3/5: Loss=0.2190, Acc=0.767\n",
      "Epoch 4/5: Loss=0.1612, Acc=0.811\n",
      "Epoch 5/5: Loss=0.1675, Acc=0.938\n",
      "\n",
      "==================================================\n",
      "REPRESENTATION: seg_mask\n",
      "==================================================\n",
      "\n",
      "Training Simple CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752752543.117589   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "I0000 00:00:1752752543.121260   39470 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3 Pro\n",
      "W0000 00:00:1752752543.127305  127868 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752752543.127988  127881 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752752543.131767  127868 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1752752543.132273  127887 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Loss=0.7386, Acc=0.815\n",
      "Epoch 2/5: Loss=0.3794, Acc=0.920\n",
      "Epoch 3/5: Loss=0.2266, Acc=0.959\n",
      "Epoch 4/5: Loss=0.1617, Acc=0.952\n",
      "Epoch 5/5: Loss=0.1571, Acc=0.968\n",
      "\n",
      "Training ResNet CNN...\n",
      "Epoch 1/5: Loss=0.3604, Acc=0.685\n",
      "Epoch 2/5: Loss=0.1526, Acc=0.954\n",
      "Epoch 3/5: Loss=0.0946, Acc=0.331\n",
      "Epoch 4/5: Loss=0.1067, Acc=0.968\n",
      "Epoch 5/5: Loss=0.0698, Acc=0.984\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # Load dataset\n",
    "    image_paths, labels, _ = explore_rps_dataset(DATASET_PATH)\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "        image_paths, labels_encoded, test_size=0.2, stratify=labels_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # results = {}\n",
    "    \n",
    "    for rep in REPRESENTATIONS[2:]:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"REPRESENTATION: {rep}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Define architectures for this representation\n",
    "        if rep == 'landmarks':\n",
    "            architectures = {\n",
    "                'Simple MLP': LandmarkMLP(input_size=42, num_classes=3),\n",
    "                #'Deep MLP': DeepLandmarkMLP(input_size=42, num_classes=3)\n",
    "            }\n",
    "            transform = None\n",
    "            batch_size = 32\n",
    "        else:\n",
    "            in_channels = 3 if rep == 'raw_rgb' else 1\n",
    "            architectures = {\n",
    "                'Simple CNN': SimpleCNN(in_channels=in_channels, num_classes=3),\n",
    "                #'Deep CNN': DeepCNN(in_channels=in_channels, num_classes=3),\n",
    "                'ResNet CNN': ResNetCNN(in_channels=in_channels, num_classes=3)\n",
    "            }\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor() if rep == 'raw_rgb' else transforms.Lambda(lambda x: x),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.Normalize(mean=[0.5]*3, std=[0.5]*3) if rep == 'raw_rgb' else transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            batch_size = 16\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = HandDataset(X_train_paths, y_train, rep, transform)\n",
    "        test_dataset = HandDataset(X_test_paths, y_test, rep, transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        results[rep] = {}\n",
    "        \n",
    "        for arch_name, model in architectures.items():\n",
    "            print(f\"\\nTraining {arch_name}...\")\n",
    "            train_losses, test_accs = train_model(model, train_loader, test_loader, epochs=5, device=device)\n",
    "            results[rep][arch_name] = {\n",
    "                'train_losses': train_losses,\n",
    "                'test_accs': test_accs,\n",
    "                'final_acc': test_accs[-1]\n",
    "            }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs587_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
