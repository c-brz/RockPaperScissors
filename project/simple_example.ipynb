{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eb5925",
   "metadata": {},
   "source": [
    "Rock-Paper-Scissors Project: using https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors/data to train a simple fully connected nn.\n",
    "\n",
    "More info: https://docs.google.com/document/d/1bluoVcS2wuhsjR2EdAjWaUpyXfjgfQ6otWEb3xfnWF8/edit?usp=sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a097d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import os\n",
    "# Redirect C-level stderr to /dev/null\n",
    "#devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "#os.dup2(devnull, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6ac55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GLOG_minloglevel\"] = \"3\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"MEDIAPIPE_DISABLE_LOG\"] = \"1\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5ec20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315fd86b",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4a4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from modules.dataset import RPSDataset\n",
    "DATASET_PATH = \"/Users/christina/.cache/kagglehub/datasets/drgfreeman/rockpaperscissors/versions/2\"\n",
    "\n",
    "# --- Setup path so imports work ---\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "from modules.dataset import *\n",
    "from modules.hand_visualizations import extract_hand_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f38738",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db3dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class RPSModel(nn.Module):\n",
    "    def __init__(self, activation_func: str = \"relu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        activation = self.get_activation(activation_func)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(63, 128),\n",
    "            activation,\n",
    "            nn.Linear(128, 64),\n",
    "            activation,\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_activation(self, name):\n",
    "        name = name.lower()\n",
    "        if name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        elif name == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        elif name == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        elif name == \"leakyrelu\":\n",
    "            return nn.LeakyReLU()\n",
    "        elif name == \"gelu\":\n",
    "            return nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87afb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load dataset and split into train/test sets\n",
    "def load_dataset(dataset_dir):\n",
    "    image_paths, labels = [], []\n",
    "\n",
    "    for label_name in LABELS:\n",
    "        class_dir = os.path.join(dataset_dir, label_name)\n",
    "        for img in os.listdir(class_dir):\n",
    "            image_paths.append(os.path.join(class_dir, img))\n",
    "            labels.append(LABELS[label_name])\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "def split_dataset(image_paths, labels, test_size=0.2, random_state=42):\n",
    "    return train_test_split(image_paths, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Training\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def visualize_prediction(model, image_path):\n",
    "    feature = extract_hand_landmarks(image_path)\n",
    "    if feature is not None:\n",
    "        x = torch.tensor(feature, dtype=torch.float32).unsqueeze(0)\n",
    "        pred = model(x).argmax(1).item()\n",
    "        label = list(LABELS.keys())[pred]\n",
    "\n",
    "        image = plt.imread(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Predicted: {label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No hand landmarks detected.\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # dataset_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'dataset'))\n",
    "    # train_paths, test_paths, train_labels, test_labels = load_dataset(dataset_dir)\n",
    "\n",
    "    dataset_dir = DATASET_PATH\n",
    "\n",
    "    print(f\"Using dataset directory: {dataset_dir}\")\n",
    "    image_paths, labels = load_dataset(dataset_dir)\n",
    "    train_paths, test_paths, train_labels, test_labels = split_dataset(image_paths, labels)\n",
    "\n",
    "    print(f\"Train set size: {len(train_paths)}\")\n",
    "    train_ds = RPSDataset(train_paths, train_labels)\n",
    "    print(f\"Test set size: {len(test_paths)}\")\n",
    "    test_ds = RPSDataset(test_paths, test_labels)\n",
    "\n",
    "    print(\"Creating DataLoaders...\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "    print(\"Creating model, criterion, and optimizer...\")\n",
    "    model = RPSModel(activation_func=\"relu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Starting training...\\n\")\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "        print(f\"Epoch {epoch + 1}: Loss={train_loss:.4f} | Train Acc={train_acc:.4f} | Test Acc={test_acc:.4f}\")\n",
    "\n",
    "    # Visualize one test prediction\n",
    "    print(\"\\nPrediction example:\")\n",
    "    visualize_prediction(model, test_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd226aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs587_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
